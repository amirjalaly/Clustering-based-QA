{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 5  # Number of epochs to train for.\n",
    "latent_dim = 200  # Latent dimensionality of the encoding space.\n",
    "max_vocab = 30000\n",
    "embeding_output_dim = 50\n",
    "CLUSTERING_METHOD =1 #0: Kmeans, 1: NMF, 2: LDA\n",
    "NUMBER_OF_CLUSTERS=3\n",
    "CLUSTERING_METHOD_NAME = ['Kmeans', 'NMF','LDA'][CLUSTERING_METHOD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from random import choices,shuffle\n",
    "from os import path\n",
    "import tensorflow  as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense,GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional,Concatenate\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because this data is private and there is no permission to publish it publicly, only the indexed data is used here.\n",
    "\n",
    "#load tokenized Questions train,validation,test \n",
    "Q_train_tokenized = pickle.load(open('data\\\\Q_train_tokenized.pkl','rb'))\n",
    "Q_valid_tokenized = pickle.load(open('data\\\\Q_valid_tokenized.pkl','rb'))\n",
    "Q_test_tokenized = pickle.load(open('data\\\\Q_test_tokenized.pkl','rb'))\n",
    "#load tokenized Answers train,validation,test\n",
    "A_train_tokenized = pickle.load(open('data\\\\A_train_tokenized.pkl','rb'))\n",
    "A_valid_tokenized = pickle.load(open('data\\\\A_valid_tokenized.pkl','rb'))\n",
    "A_test_tokenized = pickle.load(open('data\\\\A_test_tokenized.pkl','rb'))\n",
    "\n",
    "#load TFIDF vectors train,validation,test (include Question-Answer pair)\n",
    "train_TFIDFvec = pickle.load(open('data\\\\train_TFIDFvec.pkl','rb'))\n",
    "valid_TFIDFvec = pickle.load(open('data\\\\valid_TFIDFvec.pkl','rb'))\n",
    "test_TFIDFvec = pickle.load(open('data\\\\test_TFIDFvec.pkl','rb'))\n",
    "\n",
    "SOS = 2\n",
    "EOS = 3\n",
    "data_length = len(Q_train_tokenized)+len(Q_valid_tokenized)+len(Q_test_tokenized)\n",
    "train_count = len(Q_train_tokenized)\n",
    "print('total records %i, %i train, %i validation %i test'%(data_length\n",
    "                                                           ,len(Q_train_tokenized),len(Q_valid_tokenized),len(Q_test_tokenized)))\n",
    "\n",
    "num_encoder_tokens = min(max([max(Q) for Q in Q_train_tokenized if len(Q)>0]),max_vocab)+2\n",
    "num_decoder_tokens = min(max([max(A) for A in A_train_tokenized if len(A)>0]),max_vocab)+2\n",
    "print('max tokens:',num_encoder_tokens,num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the clustering on Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('clustering dataset using %s'%CLUSTERING_METHOD_NAME)\n",
    "\n",
    "#_matrix = vec.fit_transform(texts)\n",
    "#vec = CountVectorizer()\n",
    "feature_matrix = vstack((train_TFIDFvec, valid_TFIDFvec))\n",
    "\n",
    "clustering_switcher = {\n",
    "    0:KMeans(n_clusters=NUMBER_OF_CLUSTERS),\n",
    "    1:NMF(n_components=NUMBER_OF_CLUSTERS),\n",
    "    2:LatentDirichletAllocation(n_components=NUMBER_OF_CLUSTERS)\n",
    "}\n",
    "\n",
    "clustering = clustering_switcher[CLUSTERING_METHOD]\n",
    "clustering.fit(feature_matrix)\n",
    "print('Clustering method fitted to dataset')\n",
    "\n",
    "\n",
    "train_cluster_labels = clustering.transform(train_TFIDFvec) \n",
    "valid_cluster_labels = clustering.transform(valid_TFIDFvec) \n",
    "test_cluster_labels = clustering.transform(test_TFIDFvec) \n",
    "\n",
    "\n",
    "if CLUSTERING_METHOD != 0:\n",
    "    train_cluster_labels = np.argmax(train_cluster_labels,axis=-1)\n",
    "    valid_cluster_labels = np.argmax(valid_cluster_labels,axis=-1)\n",
    "    test_cluster_labels = np.argmax(test_cluster_labels,axis=-1)\n",
    "else:\n",
    "    train_cluster_labels = np.argmin(train_cluster_labels,axis=-1)\n",
    "    valid_cluster_labels = np.argmin(valid_cluster_labels,axis=-1)\n",
    "    test_cluster_labels = np.argmin(test_cluster_labels,axis=-1)\n",
    "print('Clustering done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier AUX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    try:\n",
    "        targets = np.array(data).reshape(-1)\n",
    "        output = np.zeros((len(targets),nb_classes))\n",
    "        for i,d in enumerate(targets):\n",
    "            output[i,d]=1\n",
    "        return output\n",
    "    except Exception as ex:\n",
    "        print(\"error in indices_to_one_hot\")\n",
    "        print(ex)\n",
    "        print(nb_classes)\n",
    "        print(targets)\n",
    "def generate_data_classification(data, batch_size):\n",
    "    idx =0\n",
    "    max_batch = len(data)//batch_size\n",
    "    while True:\n",
    "        idx = (idx)% max_batch\n",
    "        input_sequences = []\n",
    "        target = []\n",
    "        for q,l in data[idx * batch_size:(idx + 1) * batch_size]:\n",
    "            input_sequences.append(q)\n",
    "            target.append(l)\n",
    "\n",
    "        max_encoder_seq_length = max([len(txt) for txt in input_sequences])\n",
    "        encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length)\n",
    "        output = indices_to_one_hot(target,NUMBER_OF_CLUSTERS)\n",
    "        idx+=1\n",
    "        yield (np.array(encoder_input_data),np.array(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing  Classifier Training Data using stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_class_count = np.max([sum(train_cluster_labels==i) for i in range(NUMBER_OF_CLUSTERS)])\n",
    "train_data_bal = []\n",
    "for j in range(NUMBER_OF_CLUSTERS):\n",
    "    train_data_bal += choices([(q,l) for q,l in zip(Q_train_tokenized,train_cluster_labels) if l == j],k=max_class_count)\n",
    "shuffle(train_data_bal)\n",
    "print(\"train_data_bal : %i#\"%len(train_data_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Question Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #create classifier model\n",
    "    encoder_inputs = Input(shape=(None, ))\n",
    "    print(encoder_inputs.shape)\n",
    "    embed_encoder = Embedding(\n",
    "                        input_dim=num_encoder_tokens,\n",
    "                        output_dim=embeding_output_dim)(encoder_inputs)\n",
    "    encoder = GRU(latent_dim, dropout=0.5)\n",
    "    encoder_outputs = encoder(embed_encoder)\n",
    "\n",
    "\n",
    "    decoder_dense = Dense(NUMBER_OF_CLUSTERS, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(encoder_outputs)\n",
    "    classifier = Model(encoder_inputs, decoder_outputs)\n",
    "    classifier.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    best = 0\n",
    "    #run untill it can improve the accuracy on validation data\n",
    "    while(True):\n",
    "        classifier.fit_generator(generator=generate_data_classification(train_data_bal,batch_size),\n",
    "                        steps_per_epoch = len(Q_train_tokenized)//batch_size,\n",
    "                        epochs=5)\n",
    "        acc = classifier.evaluate_generator(generator=generate_data_classification(list(zip(Q_valid_tokenized,valid_cluster_labels)),batch_size),\n",
    "                     steps= len(Q_valid_tokenized)//batch_size)\n",
    "        acc=acc[1]\n",
    "        print('Accuracy on validation data: ',acc)\n",
    "\n",
    "        if acc > best:\n",
    "            best = acc\n",
    "            #Saveing the best so far classifier\n",
    "            classifier.save('cached_models\\\\classifier.h5')\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            #stp training since it could not improve the accuracy any more\n",
    "            break\n",
    "\n",
    "\n",
    "    print(\"training done, running on data...\")\n",
    "    #loading the best classifier\n",
    "    print('loading classifier from cached model...')\n",
    "    classifier = load_model('cached_models\\\\classifier.h5')\n",
    "    \n",
    "    print('checking accuracy on test data....')\n",
    "    print(classifier.evaluate_generator(generator=generate_data_classification(list(zip(Q_test_tokenized,test_cluster_labels)),batch_size),\n",
    "                         steps= len(Q_test_tokenized)//batch_size))\n",
    "\n",
    "    #predicting the class of dataset\n",
    "    print('running on dataset...')\n",
    "    train_cluster_predicted_labels = classifier.predict(next(generate_data_classification(list(zip(Q_train_tokenized,train_cluster_labels)),len(Q_train_tokenized)))[0])\n",
    "    train_cluster_predicted_labels = train_cluster_predicted_labels.argmax(axis=-1)\n",
    "    \n",
    "    valid_cluster_predicted_labels = classifier.predict(next(generate_data_classification(list(zip(Q_valid_tokenized,valid_cluster_labels)),len(Q_valid_tokenized)))[0])\n",
    "    valid_cluster_predicted_labels = valid_cluster_predicted_labels.argmax(axis=-1)\n",
    "    \n",
    "    test_cluster_predicted_labels = classifier.predict(next(generate_data_classification(list(zip(Q_test_tokenized,test_cluster_labels)),len(Q_test_tokenized)))[0])\n",
    "    test_cluster_predicted_labels = test_cluster_predicted_labels.argmax(axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_prob = [sum([1 for l in train_cluster_predicted_labels if l == i])/len(train_cluster_predicted_labels) for i in range(NUMBER_OF_CLUSTERS)]\n",
    "_train_data=[]\n",
    "_valid_data = []\n",
    "_test_data = []\n",
    "for index in  range(NUMBER_OF_CLUSTERS):\n",
    "    _train_data.append([(q,a) for q,a,l in zip(Q_train_tokenized,A_train_tokenized,train_cluster_predicted_labels) if l == index])\n",
    "    _valid_data.append([(q,a) for q,a,l in zip(Q_valid_tokenized,A_valid_tokenized,valid_cluster_predicted_labels) if l == index])\n",
    "    _test_data.append([(q,a) for q,a,l in zip(Q_test_tokenized,A_test_tokenized,test_cluster_predicted_labels) if l == index])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_data(data, batch_size,Infinite=True,Random=False):\n",
    "    max_batch = len(data)//batch_size\n",
    "    idx = random.randint(0,max_batch) if Random else 0\n",
    "    while Infinite or idx<=max_batch:\n",
    "        idx = (idx)% max_batch if Infinite else idx\n",
    "        input_sequences = []\n",
    "        target_sequences = []\n",
    "        for input_text, target_text in data[idx * batch_size:(idx + 1) * batch_size]:\n",
    "                input_sequences.append(input_text)\n",
    "                target_sequences.append([SOS] + target_text + [EOS])\n",
    "\n",
    "\n",
    "        max_encoder_seq_length = max([1]+[len(txt) for txt in input_sequences])\n",
    "        max_decoder_seq_length = max([1]+[len(txt) for txt in target_sequences])\n",
    "        encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length)\n",
    "        decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length,padding='post')\n",
    "        decoder_target_data = np.array([indices_to_one_hot(np.concatenate((a[1:],[0])),num_decoder_tokens) for a in decoder_input_data])\n",
    "        idx+=1\n",
    "        yield ([np.array(encoder_input_data),np.array(decoder_input_data)], np.array(decoder_target_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model for each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional,Concatenate\n",
    "from os import path\n",
    "models =  {}\n",
    "encoder_inputs = Input(shape=(None, ))\n",
    "print(encoder_inputs.shape)\n",
    "embed_encoder = Embedding(\n",
    "                    input_dim=num_encoder_tokens,\n",
    "                    output_dim=embeding_output_dim)(encoder_inputs)\n",
    "encoder = Bidirectional(GRU(latent_dim, return_state=True,dropout=0.5))\n",
    "encoder_outputs, forward_h,backward_h = encoder(embed_encoder)\n",
    "state_h = Concatenate()([forward_h,backward_h])\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "embed_decoder = Embedding(num_decoder_tokens, embeding_output_dim)(decoder_inputs)\n",
    "decoder_gru = GRU(latent_dim*2,return_sequences=True,return_state=True, dropout=0.5)\n",
    "gru_outputs,_ = decoder_gru(embed_decoder, initial_state=state_h)\n",
    "def get_model(i):\n",
    "    if i in models:\n",
    "        return models[i]\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(gru_outputs)\n",
    "\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    models[i] = model\n",
    "    return models[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extact Test Model of each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_test_models():\n",
    "    global idea_encoder,decoder\n",
    "    _encoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "\n",
    "    embed_encoder = get_model(0).layers[1](_encoder_inputs)\n",
    "    encoder = get_model(0).layers[3]\n",
    "    encoder_outputs, forward_h,backward_h = encoder(embed_encoder)\n",
    "    state_h = Concatenate()([forward_h,backward_h])\n",
    "    idea_encoder = Model(_encoder_inputs, state_h)\n",
    "    decoder = {}\n",
    "def get_decoder(i):\n",
    "    if i in decoder:\n",
    "        return decoder[i]\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "    decoder_inputs = Input(shape=(None, ))\n",
    "    embed_decoder = get_model(i).layers[4](decoder_inputs)\n",
    "    decoder_outputs, state_h = get_model(i).layers[6](\n",
    "    embed_decoder, initial_state=decoder_state_input_h)\n",
    "    decoder_states = state_h\n",
    "    decoder_outputs = get_model(i).layers[7](decoder_outputs)\n",
    "    decoder[i] = Model(\n",
    "    [decoder_inputs] + [decoder_state_input_h],\n",
    "    [decoder_outputs] + [decoder_states])\n",
    "    return decoder[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from scipy.stats import entropy\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from collections import defaultdict\n",
    "\n",
    "AllTrue = 0\n",
    "AllPred = 0\n",
    "Correct = 0\n",
    "Measures = defaultdict(lambda:[])\n",
    "Rouge1 = []\n",
    "Rouge2 = []\n",
    "RougeL = []\n",
    "Bleu1 = []\n",
    "Bleu2 = []\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rougeL'], use_stemmer=False)\n",
    "\n",
    "\n",
    "def compute_recall_precision(y_true, y_pred):\n",
    "    global AllTrue,AllPred,Correct\n",
    "    reference = [m for m in np.argmax(y_true,axis=-1) if m >0]\n",
    "    candidate = [m for m in np.argmax(y_pred,axis=-1) if m >0]\n",
    "    scores = scorer.score(' '.join([str(i) for i in reference]),\n",
    "                      ' '.join([str(i) for i in candidate]))\n",
    "\n",
    "    Measures['Entrophy'].append(-tf.reduce_sum(y_pred * tf.math.log(y_pred)))\n",
    "    Measures['Rouge1.precision'].append(scores['rouge1'].precision)\n",
    "    Measures['Rouge1.recall'].append(scores['rouge1'].recall)\n",
    "    Measures['Rouge1.fmeasure'].append(scores['rouge1'].fmeasure)\n",
    "    Measures['Rouge2.precision'].append(scores['rouge2'].precision)\n",
    "    Measures['Rouge2.recall'].append(scores['rouge2'].recall)\n",
    "    Measures['Rouge2.fmeasure'].append(scores['rouge2'].fmeasure)\n",
    "    Measures['RougeL.precision'].append(scores['rougeL'].precision)\n",
    "    Measures['RougeL.recall'].append(scores['rougeL'].recall)\n",
    "    Measures['RougeL.fmeasure'].append(scores['rougeL'].fmeasure)    \n",
    "    Measures['Bleu1'].append(sentence_bleu([reference], candidate, weights=(1,0)))\n",
    "    Measures['Bleu2'].append(sentence_bleu([reference], candidate, weights=(0, 1)))\n",
    "    \n",
    "    A = set(reference)\n",
    "    B = set(candidate)\n",
    "    AllTrue+=len(A)\n",
    "    AllPred+=len(B)\n",
    "    Correct+=len(A.intersection(B))\n",
    "    \n",
    "#    Sum += (float)(K.mean(K.equal(K.argmax(y_true, axis=-1),\n",
    "#                  K.argmax(y_pred, axis=-1))))*len(y_true)\n",
    "#    Count+=len(y_true)\n",
    "def reset_measure():\n",
    "    global AllTrue,AllPred,Correct\n",
    "    AllTrue = 0\n",
    "    AllPred = 0\n",
    "    Correct = 0\n",
    "    Entrophy = []\n",
    "    \n",
    "def compute_measure():\n",
    "    global AllTrue,AllPred,Correct\n",
    "    Recall = Correct/(AllTrue+.00000000000000000000001)\n",
    "    Precision = Correct/(AllPred+.00000000000000000000001)\n",
    "    measure = 2*Precision*Recall/(Precision+Recall+.00000000000000000000001)\n",
    "    print('Precision:\\t%.2f%%'%(Precision*100))\n",
    "    print('Recall:\\t%.2f%%'%(Recall*100))\n",
    "    print('F-measure:\\t%.2f%%'%(measure*100))\n",
    "    for key in Measures:\n",
    "        print('%s:\\t%.2f%%'%(key,np.mean(Measures[key])*100))\n",
    "    return measure\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(encoder_model,decoder_model,encoder_input_data,max_len):\n",
    "    states_value = encoder_model.predict(encoder_input_data)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0]= SOS\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    output = []\n",
    "    decoded = 0\n",
    "    while not stop_condition:\n",
    "        #print([target_seq] + [states_value])\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + [states_value])\n",
    "\n",
    "        # Sample a token\n",
    "        decoded+=1\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        output.append(output_tokens[0][0])\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (decoded >= max_len or sampled_token_index == EOS):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq[0, 0]= sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = h\n",
    "\n",
    "    return np.array([output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len= 100\n",
    "def test_proposed(data,seperated=False):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    reset_measure()\n",
    "    i =0 \n",
    "    failed =0\n",
    "    tot_len = sum([len(d) for d in data])\n",
    "    for index in tqdm(range(NUMBER_OF_CLUSTERS)):\n",
    "\n",
    "        if seperated:\n",
    "            reset_measure()\n",
    "        for X,Y in tqdm(generate_data(data[index],1,False)):\n",
    "            i+=1\n",
    "            try:\n",
    "                y_pred = run_model(idea_encoder,get_decoder(index),X[0],max_len)\n",
    "                compute_recall_precision(Y[0],y_pred[0])\n",
    "\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                failed+=1    \n",
    "        if seperated:\n",
    "                    print()\n",
    "                    compute_measure()\n",
    "    return compute_measure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    reset_test_models()\n",
    "    steps = len(Q_train_tokenized)//batch_size*epochs\n",
    "    best = 0\n",
    "    while(True):\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        for k in tqdm(range(steps)):\n",
    "            index = np.random.choice(NUMBER_OF_CLUSTERS,1,p=clusters_prob)[0]\n",
    "            hist = get_model(index).fit_generator(generator=generate_data(_train_data[index],batch_size,Random=True),\n",
    "                            steps_per_epoch = 1,#1/len(_train_data)//batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=0)\n",
    "            loss += hist.history['loss'][0]\n",
    "            accuracy += hist.history['accuracy'][0]\n",
    "\n",
    "\n",
    "        m = test_proposed(_valid_data)\n",
    "\n",
    "        if m > best:\n",
    "            best = m\n",
    "            for i in range(NUMBER_OF_CLUSTERS):\n",
    "                get_model(i).save('cached_models\\\\model%i.h5'%i)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('loading models from file')\n",
    "reset_test_models()\n",
    "for i in range(NUMBER_OF_CLUSTERS):    \n",
    "        models[i] = load_model('cached_models\\\\model%i.h5'%i)\n",
    "        # models[i].summary()\n",
    "test_proposed(_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = None\n",
    "def get_basemodel():\n",
    "    global basemodel\n",
    "    if basemodel != None:\n",
    "        return basemodel\n",
    "    \n",
    "    encoder_inputs = Input(shape=(None, ))\n",
    "    embed_encoder = Embedding(\n",
    "                        input_dim=num_encoder_tokens,\n",
    "                        output_dim=embeding_output_dim)(encoder_inputs)\n",
    "    encoder = Bidirectional(GRU(latent_dim, return_state=True,dropout=0.5))\n",
    "    encoder_outputs, forward_h,backward_h = encoder(embed_encoder)\n",
    "    state_h = Concatenate()([forward_h,backward_h])\n",
    "\n",
    "\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, ))\n",
    "    embed_decoder = Embedding(num_decoder_tokens, embeding_output_dim)(decoder_inputs)\n",
    "    decoder_gru = GRU(latent_dim*2,return_sequences=True,return_state=True, dropout=0.5)\n",
    "    gru_outputs,_ = decoder_gru(embed_decoder, initial_state=state_h)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(gru_outputs)\n",
    "\n",
    "\n",
    "    basemodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    basemodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return basemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_encoder = None\n",
    "base_decoder = None\n",
    "def get_base_encoder():\n",
    "    global base_encoder\n",
    "    if base_encoder != None:\n",
    "        return base_encoder\n",
    "    model = get_basemodel()\n",
    "    encoder_inputs = Input(shape=(None, ))\n",
    "    embed_encoder = model.layers[1](encoder_inputs)\n",
    "    encoder = model.layers[3]\n",
    "    encoder_outputs, forward_h,backward_h = encoder(embed_encoder)\n",
    "    state_h = Concatenate()([forward_h,backward_h])\n",
    "    base_encoder = Model(encoder_inputs, state_h)\n",
    "    return base_encoder\n",
    "\n",
    "def get_base_decoder():\n",
    "    global base_decoder\n",
    "    if base_decoder != None:\n",
    "        return base_decoder\n",
    "    model = get_basemodel()\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "    decoder_inputs = Input(shape=(None, ))\n",
    "    embed_decoder = model.layers[4](decoder_inputs)\n",
    "    decoder_outputs, state_h = model.layers[6](\n",
    "        embed_decoder, initial_state=decoder_state_input_h)\n",
    "    decoder_states = state_h\n",
    "    decoder_outputs = model.layers[7](decoder_outputs)\n",
    "    base_decoder = Model(\n",
    "        [decoder_inputs] + [decoder_state_input_h],\n",
    "        [decoder_outputs] + [decoder_states])\n",
    "    return base_decoder\n",
    "def test_base(data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    reset_measure()\n",
    "    i =0 \n",
    "    failed =0\n",
    "    for X,Y in tqdm( generate_data(data,1,False)):\n",
    "        i+=1\n",
    "        try:\n",
    "            y_pred = run_model(get_base_encoder(),get_base_decoder(),X[0],max_len)\n",
    "            compute_recall_precision(Y[0],y_pred[0])\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            failed+=1\n",
    "    return compute_measure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    best = 0\n",
    "    while(True):\n",
    "        get_basemodel().fit_generator(generator=generate_data(list(zip(Q_train_tokenized,A_train_tokenized)),batch_size,Random=False),\n",
    "                            steps_per_epoch = len(Q_train_tokenized)//batch_size,\n",
    "                            epochs=epochs)\n",
    "\n",
    "        m= test_base(list(zip(Q_valid_tokenized,A_valid_tokenized)))\n",
    "        if m>best:\n",
    "            best = m\n",
    "            get_basemodel().save('cached_models\\\\model_base.h5')\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel=load_model('cached_models\\\\model_base.h5')\n",
    "base_encoder = None\n",
    "base_decoder = None\n",
    "test_base(list(zip(Q_test_tokenized,A_test_tokenized)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
